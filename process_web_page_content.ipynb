{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "web_pages_content = json.load(open(\"web_pages_content.json\"))\n",
    "print(len(web_pages_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = \"\"\"## On this page\"\"\" \n",
    "for web_page in web_pages_content:\n",
    "    text = web_pages_content[web_page]\n",
    "    if marker in text:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            web_pages_content[web_page] = text[(idx+len(marker)):].strip()\n",
    "        else:\n",
    "            print(web_page,\"** no marker text **\")\n",
    "    else:\n",
    "        print(web_page,\"** no marker text **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"web_pages_content_processed.json\", \"w\") as f:\n",
    "    json.dump(web_pages_content,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whether to chunk or not?\n",
    "Let's check if each web page is withing the token limit of openai embedding model.<br>\n",
    "If it is, we will not chunk it. If not we will chunk it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 8191\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model('text-embedding-3-large')\n",
    "def num_tokens_from_string(string: str)->int:\n",
    "    return len(encoding.encode(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.chaicode.com/youtube/chai-aur-c/control-flow/ Num tokens: 3415\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/data-types/ Num tokens: 2648\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/functions/ Num tokens: 1821\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/hello-world/ Num tokens: 1069\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/introduction/ Num tokens: 2081\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/loops/ Num tokens: 2677\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/operators/ Num tokens: 3287\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/variables-and-constants/ Num tokens: 1028\n",
      "https://docs.chaicode.com/youtube/chai-aur-c/welcome/ Num tokens: 290\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/nginx-rate-limiting/ Num tokens: 1169\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/nginx-ssl-setup/ Num tokens: 1199\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/node-logger/ Num tokens: 1283\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/node-nginx-vps/ Num tokens: 1173\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/postgresql-docker/ Num tokens: 1662\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/postgresql-vps/ Num tokens: 1387\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/setup-nginx/ Num tokens: 1106\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/setup-vpc/ Num tokens: 2174\n",
      "https://docs.chaicode.com/youtube/chai-aur-devops/welcome/ Num tokens: 290\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/getting-started/ Num tokens: 2326\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/jinja-templates/ Num tokens: 2868\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/models/ Num tokens: 2038\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/relationships-and-forms/ Num tokens: 1872\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/tailwind/ Num tokens: 1527\n",
      "https://docs.chaicode.com/youtube/chai-aur-django/welcome/ Num tokens: 281\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/behind-the-scenes/ Num tokens: 933\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/branches/ Num tokens: 1822\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/diff-stash-tags/ Num tokens: 2247\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/github/ Num tokens: 2025\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/introduction/ Num tokens: 1075\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/managing-history/ Num tokens: 1226\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/terminology/ Num tokens: 1991\n",
      "https://docs.chaicode.com/youtube/chai-aur-git/welcome/ Num tokens: 281\n",
      "https://docs.chaicode.com/youtube/chai-aur-html/emmit-crash-course/ Num tokens: 1083\n",
      "https://docs.chaicode.com/youtube/chai-aur-html/html-tags/ Num tokens: 2222\n",
      "https://docs.chaicode.com/youtube/chai-aur-html/introduction/ Num tokens: 954\n",
      "https://docs.chaicode.com/youtube/chai-aur-html/welcome/ Num tokens: 276\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/database-design-exercise/ Num tokens: 1348\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/introduction/ Num tokens: 747\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/joins-and-keys/ Num tokens: 1861\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/joins-exercise/ Num tokens: 648\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/normalization/ Num tokens: 1435\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/postgres/ Num tokens: 852\n",
      "https://docs.chaicode.com/youtube/chai-aur-sql/welcome/ Num tokens: 330\n",
      "https://docs.chaicode.com/youtube/getting-started/ Num tokens: 614\n"
     ]
    }
   ],
   "source": [
    "with open(\"web_pages_content_processed.json\", \"r\") as f:\n",
    "    web_pages_content = json.load(f)\n",
    "\n",
    "for web_page in web_pages_content:\n",
    "    text = web_pages_content[web_page]\n",
    "    if num_tokens_from_string(text) > max_tokens:\n",
    "        print(web_page,\"** token limit exceeded **\")\n",
    "    else:\n",
    "        print(web_page,f\"Num tokens: {num_tokens_from_string(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All chunks are within the token limit of openai embedding model. So we will not chunk them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatdocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
