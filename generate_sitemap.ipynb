{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitemapGenerator:\n",
    "    def __init__(self, base_url, max_pages=500, delay=1):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        self.visited_urls = set()\n",
    "        self.to_visit = deque([base_url])\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay  # Delay between requests to be respectful\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL belongs to the same domain and is valid\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return (\n",
    "            parsed.netloc == self.domain and\n",
    "            parsed.scheme in ['http', 'https'] and\n",
    "            not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.css', '.js', '.ico'])\n",
    "        )\n",
    "    \n",
    "    def get_links_from_page(self, url):\n",
    "        \"\"\"Extract all links from a webpage\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = set()\n",
    "            \n",
    "            # Find all anchor tags with href attributes\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href'].strip()\n",
    "                if href:\n",
    "                    # Convert relative URLs to absolute URLs\n",
    "                    absolute_url = urljoin(url, href)\n",
    "                    # Remove fragments (anchors)\n",
    "                    absolute_url = absolute_url.split('#')[0]\n",
    "                    if self.is_valid_url(absolute_url):\n",
    "                        links.add(absolute_url)\n",
    "            \n",
    "            return links\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {str(e)}\")\n",
    "            return set()\n",
    "    \n",
    "    def crawl_website(self):\n",
    "        \"\"\"Crawl the website and collect all URLs\"\"\"\n",
    "        print(f\"Starting to crawl {self.base_url}\")\n",
    "        print(f\"Max pages: {self.max_pages}, Delay: {self.delay}s\")\n",
    "        \n",
    "        while self.to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            current_url = self.to_visit.popleft()\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Crawling ({len(self.visited_urls) + 1}/{self.max_pages}): {current_url}\")\n",
    "            \n",
    "            # Add current URL to visited\n",
    "            self.visited_urls.add(current_url)\n",
    "            \n",
    "            # Get links from current page\n",
    "            links = self.get_links_from_page(current_url)\n",
    "            \n",
    "            # Add new links to queue\n",
    "            for link in links:\n",
    "                if link not in self.visited_urls and link not in self.to_visit:\n",
    "                    self.to_visit.append(link)\n",
    "            \n",
    "            # Be respectful with delays\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        print(f\"Crawling completed. Found {len(self.visited_urls)} pages.\")\n",
    "        return sorted(list(self.visited_urls))\n",
    "    \n",
    "    def generate_xml_sitemap(self, urls, filename='sitemap.xml'):\n",
    "        \"\"\"Generate XML sitemap from URLs\"\"\"\n",
    "        # Create root element\n",
    "        urlset = ET.Element('urlset')\n",
    "        urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')\n",
    "        \n",
    "        # Add each URL\n",
    "        for url in urls:\n",
    "            url_elem = ET.SubElement(urlset, 'url')\n",
    "            \n",
    "            # Add location\n",
    "            loc_elem = ET.SubElement(url_elem, 'loc')\n",
    "            loc_elem.text = url\n",
    "            \n",
    "            # Add last modified date (current date)\n",
    "            lastmod_elem = ET.SubElement(url_elem, 'lastmod')\n",
    "            lastmod_elem.text = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Add change frequency (optional)\n",
    "            changefreq_elem = ET.SubElement(url_elem, 'changefreq')\n",
    "            changefreq_elem.text = 'weekly'\n",
    "            \n",
    "            # Add priority (optional)\n",
    "            priority_elem = ET.SubElement(url_elem, 'priority')\n",
    "            # Give homepage higher priority\n",
    "            if url == self.base_url or url == self.base_url + '/':\n",
    "                priority_elem.text = '1.0'\n",
    "            else:\n",
    "                priority_elem.text = '0.8'\n",
    "        \n",
    "        # Create tree and write to file\n",
    "        tree = ET.ElementTree(urlset)\n",
    "        ET.indent(tree, space=\"  \", level=0)  # Pretty print\n",
    "        tree.write(filename, encoding='utf-8', xml_declaration=True)\n",
    "        print(f\"XML sitemap saved to {filename}\")\n",
    "    \n",
    "    def generate_txt_sitemap(self, urls, filename='sitemap.txt'):\n",
    "        \"\"\"Generate text sitemap from URLs\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for url in urls:\n",
    "                f.write(url + '\\n')\n",
    "        print(f\"Text sitemap saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    base_url = \"https://docs.chaicode.com/\"\n",
    "    max_pages = 100  # Adjust based on site size\n",
    "    delay = 1  # Seconds between requests\n",
    "    \n",
    "    # Create generator instance\n",
    "    generator = SitemapGenerator(base_url, max_pages, delay)\n",
    "    \n",
    "    # Crawl the website\n",
    "    urls = generator.crawl_website()\n",
    "    \n",
    "    if urls:\n",
    "        # Generate both XML and text sitemaps\n",
    "        generator.generate_xml_sitemap(urls, 'sitemap.xml')\n",
    "        generator.generate_txt_sitemap(urls, 'sitemap.txt')\n",
    "        \n",
    "        print(f\"\\nSitemap generation complete!\")\n",
    "        print(f\"Total URLs found: {len(urls)}\")\n",
    "        print(f\"Files generated: sitemap.xml, sitemap.txt\")\n",
    "    else:\n",
    "        print(\"No URLs found to generate sitemap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl https://docs.chaicode.com\n",
      "Max pages: 100, Delay: 1s\n",
      "Crawling (1/100): https://docs.chaicode.com/\n",
      "Crawling (2/100): https://docs.chaicode.com/contribute/guide\n",
      "Crawling (3/100): https://docs.chaicode.com/youtube/getting-started/\n",
      "Crawling (4/100): https://docs.chaicode.com/contribute/starter-kit/managing-assets/\n",
      "Crawling (5/100): https://docs.chaicode.com/contribute/starter-kit/authoring-content/\n",
      "Crawling (6/100): https://docs.chaicode.com/contribute/starter-kit/mdx-crash-course/\n",
      "Crawling (7/100): https://docs.chaicode.com/contribute/starter-kit/components-library/\n",
      "Crawling (8/100): https://docs.chaicode.com\n",
      "Crawling (9/100): https://docs.chaicode.com/contribute/guide/\n",
      "Crawling (10/100): https://docs.chaicode.com/contribute/starter-kit/adding-contnet/\n",
      "Crawling (11/100): https://docs.chaicode.com/contribute/starter-kit/contributing-workflow/\n",
      "Crawling (12/100): https://docs.chaicode.com/contribute/starter-kit/next-step/\n",
      "Crawling (13/100): https://docs.chaicode.com/contribute/starter-kit/project-structure/\n",
      "Crawling (14/100): https://docs.chaicode.com/contribute/starter-kit/page-metadata/\n",
      "Crawling (15/100): https://docs.chaicode.com/youtube/chai-aur-django/getting-started/\n",
      "Crawling (16/100): https://docs.chaicode.com/youtube/chai-aur-devops/node-logger/\n",
      "Crawling (17/100): https://docs.chaicode.com/youtube/chai-aur-devops/welcome/\n",
      "Crawling (18/100): https://docs.chaicode.com/youtube/chai-aur-django/jinja-templates/\n",
      "Crawling (19/100): https://docs.chaicode.com/youtube/chai-aur-django/welcome/\n",
      "Crawling (20/100): https://docs.chaicode.com/youtube/chai-aur-html/introduction/\n",
      "Crawling (21/100): https://docs.chaicode.com/youtube/chai-aur-html/welcome/\n",
      "Crawling (22/100): https://docs.chaicode.com/youtube/chai-aur-c/operators/\n",
      "Crawling (23/100): https://docs.chaicode.com/youtube/chai-aur-git/welcome/\n",
      "Crawling (24/100): https://docs.chaicode.com/youtube/chai-aur-git/introduction/\n",
      "Crawling (25/100): https://docs.chaicode.com/youtube/chai-aur-django/tailwind/\n",
      "Crawling (26/100): https://docs.chaicode.com/youtube/chai-aur-django/relationships-and-forms/\n",
      "Crawling (27/100): https://docs.chaicode.com/youtube/chai-aur-c/control-flow/\n",
      "Crawling (28/100): https://docs.chaicode.com/youtube/chai-aur-sql/database-design-exercise/\n",
      "Crawling (29/100): https://docs.chaicode.com/youtube/chai-aur-devops/node-nginx-vps/\n",
      "Crawling (30/100): https://docs.chaicode.com/youtube/chai-aur-devops/postgresql-docker/\n",
      "Crawling (31/100): https://docs.chaicode.com/youtube/chai-aur-devops/postgresql-vps/\n",
      "Crawling (32/100): https://docs.chaicode.com/youtube/chai-aur-git/terminology/\n",
      "Crawling (33/100): https://docs.chaicode.com/youtube/chai-aur-c/hello-world/\n",
      "Crawling (34/100): https://docs.chaicode.com/youtube/chai-aur-sql/introduction/\n",
      "Crawling (35/100): https://docs.chaicode.com/youtube/chai-aur-c/introduction/\n",
      "Crawling (36/100): https://docs.chaicode.com/youtube/chai-aur-sql/welcome/\n",
      "Crawling (37/100): https://docs.chaicode.com/youtube/chai-aur-sql/joins-exercise/\n",
      "Crawling (38/100): https://docs.chaicode.com/youtube/chai-aur-html/html-tags/\n",
      "Crawling (39/100): https://docs.chaicode.com/youtube/chai-aur-git/managing-history/\n",
      "Crawling (40/100): https://docs.chaicode.com/youtube/chai-aur-c/functions/\n",
      "Crawling (41/100): https://docs.chaicode.com/youtube/chai-aur-c/variables-and-constants/\n",
      "Crawling (42/100): https://docs.chaicode.com/youtube/chai-aur-sql/normalization/\n",
      "Crawling (43/100): https://docs.chaicode.com/youtube/chai-aur-devops/setup-vpc/\n",
      "Crawling (44/100): https://docs.chaicode.com/youtube/chai-aur-html/emmit-crash-course/\n",
      "Crawling (45/100): https://docs.chaicode.com/youtube/chai-aur-git/diff-stash-tags/\n",
      "Crawling (46/100): https://docs.chaicode.com/youtube/chai-aur-c/loops/\n",
      "Crawling (47/100): https://docs.chaicode.com/youtube/chai-aur-devops/setup-nginx/\n",
      "Crawling (48/100): https://docs.chaicode.com/youtube/chai-aur-git/behind-the-scenes/\n",
      "Crawling (49/100): https://docs.chaicode.com/youtube/chai-aur-c/welcome/\n",
      "Crawling (50/100): https://docs.chaicode.com/youtube/chai-aur-sql/postgres/\n",
      "Crawling (51/100): https://docs.chaicode.com/youtube/chai-aur-devops/nginx-rate-limiting/\n",
      "Crawling (52/100): https://docs.chaicode.com/youtube/chai-aur-devops/nginx-ssl-setup/\n",
      "Crawling (53/100): https://docs.chaicode.com/youtube/chai-aur-c/data-types/\n",
      "Crawling (54/100): https://docs.chaicode.com/youtube/chai-aur-django/models/\n",
      "Crawling (55/100): https://docs.chaicode.com/youtube/chai-aur-sql/joins-and-keys/\n",
      "Crawling (56/100): https://docs.chaicode.com/youtube/chai-aur-git/github/\n",
      "Crawling (57/100): https://docs.chaicode.com/youtube/chai-aur-git/branches/\n",
      "Crawling (58/100): https://docs.chaicode.com/tracks\n",
      "Crawling (59/100): https://docs.chaicode.com/getting-started/\n",
      "Error crawling https://docs.chaicode.com/getting-started/: 404 Client Error: Not Found for url: https://docs.chaicode.com/getting-started/\n",
      "Crawling (60/100): https://docs.chaicode.com/contribute/starter-kit/mdx-crash-course\n",
      "Crawling (61/100): https://docs.chaicode.com/contribute/starter-kit/page-content/\n",
      "Error crawling https://docs.chaicode.com/contribute/starter-kit/page-content/: 404 Client Error: Not Found for url: https://docs.chaicode.com/contribute/starter-kit/page-content/\n",
      "Crawling (62/100): https://docs.chaicode.com/components/cards/\n",
      "Error crawling https://docs.chaicode.com/components/cards/: 404 Client Error: Not Found for url: https://docs.chaicode.com/components/cards/\n",
      "Crawling (63/100): https://docs.chaicode.com/components/link-cards/\n",
      "Error crawling https://docs.chaicode.com/components/link-cards/: 404 Client Error: Not Found for url: https://docs.chaicode.com/components/link-cards/\n",
      "Crawling (64/100): https://docs.chaicode.com/contribute/starter-kit/components-library/contribute/starter-kit/components\n",
      "Error crawling https://docs.chaicode.com/contribute/starter-kit/components-library/contribute/starter-kit/components: 404 Client Error: Not Found for url: https://docs.chaicode.com/contribute/starter-kit/components-library/contribute/starter-kit/components\n",
      "Crawling (65/100): https://docs.chaicode.com/contribute/starter-kit/meta-data/\n",
      "Error crawling https://docs.chaicode.com/contribute/starter-kit/meta-data/: 404 Client Error: Not Found for url: https://docs.chaicode.com/contribute/starter-kit/meta-data/\n",
      "Crawling (66/100): https://docs.chaicode.com/tracks/\n",
      "Crawling (67/100): https://docs.chaicode.com/cdn-cgi/l/email-protection\n",
      "Error crawling https://docs.chaicode.com/cdn-cgi/l/email-protection: 404 Client Error: Not Found for url: https://docs.chaicode.com/cdn-cgi/l/email-protection\n",
      "Crawling (68/100): https://docs.chaicode.com/data-science/getting-started\n",
      "Crawling (69/100): https://docs.chaicode.com/gen-ai/getting-started\n",
      "Crawling (70/100): https://docs.chaicode.com/devops/getting-started\n",
      "Crawling (71/100): https://docs.chaicode.com/youtube/getting-started\n",
      "Crawling (72/100): https://docs.chaicode.com/web-dev/getting-started\n",
      "Crawling (73/100): https://docs.chaicode.com/data-science/environmnet-setup/windows/\n",
      "Crawling (74/100): https://docs.chaicode.com/data-science/getting-started/\n",
      "Crawling (75/100): https://docs.chaicode.com/data-science/environmnet-setup/mac-os/\n",
      "Crawling (76/100): https://docs.chaicode.com/gen-ai/getting-started/\n",
      "Crawling (77/100): https://docs.chaicode.com/gen-ai/environmnet-setup/windows/\n",
      "Crawling (78/100): https://docs.chaicode.com/gen-ai/environmnet-setup/mac-os/\n",
      "Crawling (79/100): https://docs.chaicode.com/devops/getting-started/\n",
      "Crawling (80/100): https://docs.chaicode.com/devops/environmnet-setup/windows/\n",
      "Crawling (81/100): https://docs.chaicode.com/devops/environmnet-setup/mac-os/\n",
      "Crawling (82/100): https://docs.chaicode.com/web-dev/environmnet-setup/mac-os/\n",
      "Crawling (83/100): https://docs.chaicode.com/web-dev/environmnet-setup/windows/\n",
      "Crawling (84/100): https://docs.chaicode.com/web-dev/getting-started/\n",
      "Crawling completed. Found 84 pages.\n",
      "XML sitemap saved to sitemap.xml\n",
      "Text sitemap saved to sitemap.txt\n",
      "\n",
      "Sitemap generation complete!\n",
      "Total URLs found: 84\n",
      "Files generated: sitemap.xml, sitemap.txt\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatdocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
